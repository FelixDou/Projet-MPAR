{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "import random\n",
    "import graphviz as gv\n",
    "import imageio\n",
    "import os\n",
    "import imageio.v2 as imageio\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "class States():\n",
    "    def __init__(self, state,transitions_with_action, transitions_without_action, reward, resolution = False):\n",
    "        \"\"\"\n",
    "        Initialisation des états de la chaine de markov\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        state : str\n",
    "            Nom de l'état (S0...)\n",
    "        reward : dictionnaire des rewards\n",
    "            Récompense de l'état\n",
    "        transitions_with_action : dict\n",
    "            Dictionnaire de l'ensemble des transitions avec action\n",
    "        transitions_without_action : dict\n",
    "            Dictionnaire de l'ensemble des transitions sans action\n",
    "        resolution : bool\n",
    "            Booléen qui permet de savoir si on veut résoudre les problèmes détéctés automatiquement si possible\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        if reward != {}: # Dans le cas où on n'a pas renseigné de récompense pour un état\n",
    "            self.reward = reward[state] # récompense de l'état\n",
    "        else:\n",
    "            self.reward = 0\n",
    "        self.transitions_without_action = {}\n",
    "        self.transitions_with_action = {}\n",
    "\n",
    "        sans_action = True # on crée ce booléen pour vérifier qu'il n'y a pas de transition avec action ensuite\n",
    "        for transition in transitions_without_action:\n",
    "            # on parcours l'ensemble des transitions\n",
    "            transi_active = transitions_without_action[transition]\n",
    "            if transi_active[\"from\"]==self.state: # on vérifie si l'état de départ est l'état actif\n",
    "                sans_action = False\n",
    "                self.transitions_without_action[\"targets\"] = transi_active[\"targets\"]\n",
    "                self.transitions_without_action[\"weights\"] = transi_active[\"weights\"]\n",
    "\n",
    "        for transition in transitions_with_action:\n",
    "            transi_active = transitions_with_action[transition]\n",
    "            if transi_active[\"from\"]==self.state and sans_action == True:\n",
    "                # les clés dans transitions_with_action sont les actions et les valeurs des dict des états cibles et leurs poids\n",
    "                self.transitions_with_action[transi_active[\"action\"]] = {\"targets\" : transi_active[\"targets\"], \"weights\" : transi_active[\"weights\"]}\n",
    "            elif transi_active[\"from\"]==self.state and sans_action == False:\n",
    "                print(\"\\nWarning : l'état\", self.state, \"comporte des transitions avec et sans action\")\n",
    "                if resolution == True: # On choisit pour résoudre le problème de supprimer les transitions avec action de l'état, pour ça on ne rajoute pas la transition dans self.transitions_with_action\n",
    "                    self.transitions_with_action = {}\n",
    "                    print(\"Le problème a été résolu automatiquement : une transition avec action de l'état\", self.state, \"a été supprimée\")\n",
    "                    pass\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"State : {self.state} \\n State reward {self.reward} \\n Transitions without action : {self.transitions_without_action} \\n Transitions with action : {self.transitions_with_action}\")\n",
    "\n",
    "\n",
    "class markov():\n",
    "    def __init__(self, fichier_mdp):\n",
    "        \"\"\"\n",
    "        Initialisation de la chaine de markov\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        fichier_mdp : str\n",
    "            Nom du fichier contenant la modélisation de la chaine de markov\n",
    "        \"\"\"\n",
    "        ! python mdp_lecture/mdp.py < {fichier_mdp} # On lance le fichier mdp.py avec la modélisation voulue\n",
    "        # On lit le .pickle des données que l'on souhaitent récupérer\n",
    "        def read_list(nom_liste):\n",
    "            # for reading also binary mode is important\n",
    "            with open(nom_liste, 'rb') as fp:\n",
    "                liste = pickle.load(fp)\n",
    "                return liste\n",
    "\n",
    "        L = read_list(\"liste_donnees\")\n",
    "        # print('Données récupérées', L)\n",
    "        states, actions, transitions_with_action, transitions_without_action, reward = L['States'], L['Actions'], L['Transitions_with_action'], L['Transitions_without_action'], L['Rewards']\n",
    "        print(\"-\"*50 + \"\\n\")\n",
    "\n",
    "        # Pour savoir si on veut résoudre les problèmes détectés automatiquement\n",
    "\n",
    "        print(\"\\nSouhaitez-vous que les problèmes détéctés prochainement soient résolus automatiquement si cela est possible? (y/n)\")\n",
    "        time.sleep(0.5)\n",
    "        reponse_resolution_pb = input()\n",
    "        if reponse_resolution_pb == \"y\":\n",
    "            resolution = True\n",
    "        else:\n",
    "            resolution = False\n",
    "\n",
    "\n",
    "        # on peut alors initialiser les éléments de la chaine de markov\n",
    "        self.states = {}\n",
    "        self.liste_states = states\n",
    "        self.actions = actions\n",
    "        self.transitions_with_action = transitions_with_action\n",
    "        self.transitions_without_action = transitions_without_action\n",
    "\n",
    "        for state in states:\n",
    "            self.states[f\"{state}\"] = States(state, transitions_with_action, transitions_without_action, reward, resolution)\n",
    "            delete = [] # on crée une liste pour stocker les transitions à supprimer\n",
    "            for transition in transitions_with_action: # On supprime les transitions avec action qui ne sont pas dans l'état actif mais qui ont le même état de départ du à une correction de chaine\n",
    "                if transitions_with_action[transition][\"from\"] == state : \n",
    "                    if transitions_with_action[transition]['action'] not in self.states[f\"{state}\"].transitions_with_action : \n",
    "                        delete.append(transition)\n",
    "            for transition in delete:\n",
    "                del transitions_with_action[transition]\n",
    "\n",
    "        # On vérifie que la chaine de markov est correctement définie et on résout les problèmes si possibles et si souhaité\n",
    "        markov.parsing(self, states, resolution) # on vérifie qu'on a pas de problèmes de parsing\n",
    "\n",
    "        \n",
    "        markov.afficher(self,  etat = \"default\", file_name = \"initial\")\n",
    "        print(\"\\nLa chaine de markov a été initialisée, il est possible de la voir dans le fichier initial.png \\n\")\n",
    "        \n",
    "        print(\"Que souhaitez-vous faire ?\")\n",
    "        print(\"1. lancer un parcours\")\n",
    "        print(\"2. model checking d'un eventually\")\n",
    "        print(\"3. SMC quantitatif\")\n",
    "        print(\"4. SMC qualitatif\")\n",
    "        print(\"5. model checking d'un next\")\n",
    "        print(\"6. Algorithme d'itération de valeurs\")\n",
    "        print(\"7. Q learning\\n\")\n",
    "        time.sleep(0.5)\n",
    "        reponse = input()\n",
    "        while reponse not in [\"1\", \"2\", \"3\",\"4\", \"5\", \"6\", \"7\"]:\n",
    "            reponse = input()\n",
    "        if reponse == \"1\":\n",
    "            markov.parcours(self)\n",
    "        elif reponse == \"2\":\n",
    "            print(\"Quel sont les états cibles ? Veuillez les rentrer un par un, et taper 'fin' quand vous avez fini\")\n",
    "            time.sleep(0.5)\n",
    "            etat = input()\n",
    "            etats_cibles = []\n",
    "            while etat != \"fin\":\n",
    "                if etat not in self.liste_states:\n",
    "                    print(\"L'état\", etat, \"n'existe pas\")\n",
    "                    etat = input()\n",
    "                else :\n",
    "                    etats_cibles.append(etat)\n",
    "                    etat = input()\n",
    "            if self.actions[0] == \"None\":\n",
    "                markov.eventually_check_DTMC(self, etats_cibles)\n",
    "            else:\n",
    "                markov.eventually_check_MDP(self, etats_cibles)\n",
    "        elif reponse == \"3\" :\n",
    "            if self.transitions_with_action == {}:\n",
    "                markov.smc_quantitatif(self)\n",
    "            else :\n",
    "                print(\"SMC quantitatif impossible car la chaine de markov n'est pas déterministe\")\n",
    "                 # Il faut redemarrer le proramme pour pouvoir lancer autre chose\n",
    "        elif reponse == \"4\" :\n",
    "            if self.transitions_with_action == {}:\n",
    "                markov.smc_qualitatif(self)\n",
    "            else :\n",
    "                print(\"SMC qualitatif impossible car la chaine de markov n'est pas déterministe\")\n",
    "                 # Il faut redemarrer le proramme pour pouvoir lancer autre chose\n",
    "        elif reponse ==\"5\":\n",
    "            markov.next_check(self)\n",
    "        elif reponse == \"6\":\n",
    "            markov.algos_iterations(self)\n",
    "        elif reponse == \"7\":\n",
    "            markov.q_learning(self)\n",
    "       \n",
    "    def parsing(self, states, resolution):\n",
    "        \"\"\"\n",
    "        On réalise différents tests pour vérifier que la chaine de markov (ou la MDP) est correctement définie\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        states : list\n",
    "            Liste des états de la chaines (avec possiblement des doublons)\n",
    "        resolution : bool\n",
    "            Booléen qui permet de savoir si on veut résoudre les problèmes détéctés automatiquement si possible\n",
    "        \"\"\"\n",
    "        # On concatène les transitions avec et sans action pour plus de simplicité pour les test\n",
    "        Warning = False # passe à True si on a un warning\n",
    "\n",
    "        all_transitions = []\n",
    "        transitions_without_action = []\n",
    "        transitions_with_action = []\n",
    "        for elem in self.transitions_without_action:\n",
    "            transitions_without_action.append(self.transitions_without_action[elem])\n",
    "            all_transitions.append(self.transitions_without_action[elem])\n",
    "        for elem in self.transitions_with_action:\n",
    "            transitions_with_action.append(self.transitions_with_action[elem])\n",
    "            all_transitions.append(self.transitions_with_action[elem])\n",
    "\n",
    "        # On vérifie que les états déclarés sont utilisés et qu'un état utilisé dans des transitions est déclaré\n",
    "        used_states = []\n",
    "        for transitions in all_transitions:\n",
    "            if transitions[\"from\"] not in self.states:\n",
    "                print(f\"Warning : l'état {transitions['from']} est utilisé dans une transition mais n'est pas déclaré\")\n",
    "                Warning = True\n",
    "                if resolution:\n",
    "                    # On rajoute l'état manquant\n",
    "                    self.states[transitions[\"from\"]] = States(transitions[\"from\"], self.transitions_with_action, self.transitions_without_action)\n",
    "\n",
    "            used_states.append(transitions[\"from\"])\n",
    "            used_states.append(transitions[\"targets\"])\n",
    "            for target in transitions[\"targets\"]:\n",
    "                if target not in self.states:\n",
    "                    print(f\"Warning : l'état {target} est utilisé dans une transition mais n'est pas déclaré\")\n",
    "                    Warning = True\n",
    "                    if resolution:\n",
    "                        # On rajoute l'état manquant\n",
    "                        print(\"Rajout de l'état manquant\")\n",
    "                        self.states[target] = States(target, self.transitions_with_action, self.transitions_without_action, {})\n",
    "        for state in self.states:\n",
    "            if state not in used_states:\n",
    "                print(f\"Warning : l'état {state} est déclaré mais n'est pas utilisé\")\n",
    "                Warning = True\n",
    "                if resolution:\n",
    "                    # On rajoute une transition de l'état vers lui même pour éviter les erreurs\n",
    "                    print(\"Rajout d'une transition de l'état vers lui même\")\n",
    "                    self.transitions_without_action[f\"{state}\"] = {'from': f'{state}', 'targets': [f\"{state}\"], 'weights': [10]}\n",
    "                    self.states[f\"{state}\"].transitions_without_action = self.transitions_without_action[f\"{state}\"]\n",
    "\n",
    "        # On vérifie qu'un état n'est pas déclaré plusieurs fois\n",
    "        if len(states) != len(self.states): # self.states supprime automatiquement les doublons\n",
    "            print(\"Warning : un état est déclaré plusieurs fois\")\n",
    "            Warning = True\n",
    "\n",
    "        # On vérifie qu'un état possède bien une transition de sortie\n",
    "        etats_sans_sortie = []\n",
    "        dict_etats = self.states\n",
    "        bool_etat_sans_sortie = False\n",
    "        for state in dict_etats:\n",
    "            if dict_etats[state].transitions_without_action == {} and dict_etats[state].transitions_with_action == {}:\n",
    "                etats_sans_sortie.append(state)\n",
    "                bool_etat_sans_sortie = True\n",
    "                if resolution:\n",
    "                    # On rajoute une transition de l'état vers lui même pour pour lui créer un état de sortie\n",
    "                    self.transitions_without_action[f\"{state}\"] = {'from': f'{state}', 'targets': [f\"{state}\"], 'weights': [10]}\n",
    "                    self.states[f\"{state}\"].transitions_without_action = {'from': f'{state}', 'targets': [f\"{state}\"], 'weights': [10]}\n",
    "\n",
    "        if bool_etat_sans_sortie == True:\n",
    "            print(\"Warning : les états suivants n'ont pas de transition de sortie : \", etats_sans_sortie)\n",
    "            print(f\"Rajout d'une transition des états {etats_sans_sortie} vers eux mêmes\")\n",
    "\n",
    "        # On vérifie que les actions ne sont pas déclarées plusieurs fois\n",
    "        actions_uniques = set(self.actions)\n",
    "        if len(actions_uniques) != len(self.actions):\n",
    "            print(\"Warning : une action est déclarée plusieurs fois\")\n",
    "            Warning = True\n",
    "            if resolution:\n",
    "                print(\"Suppressions automatiques des actions déclarées plusieurs fois\")\n",
    "    \n",
    "        # On vérifie qu'une action déclarée est utilisée\n",
    "        actions_utilisés = []\n",
    "        for transitions in transitions_with_action:\n",
    "            actions_utilisés.append(transitions[\"action\"])\n",
    "        actions_utilisés = set(actions_utilisés) # on enlève les doublons\n",
    "        if len(actions_utilisés) != len(self.actions) and self.actions != [\"None\"]: # On met None de base s'il n'y a pas d'action\n",
    "            print(\"Warning : une action est déclarée mais n'est pas utilisée\")\n",
    "            Warning = True\n",
    "            if resolution:\n",
    "                print(\"Suppression des actions non utilisées\")\n",
    "                # On supprime les actions non utilisées\n",
    "                for action in self.actions:\n",
    "                    if action not in actions_utilisés:\n",
    "                        self.actions.remove(action)\n",
    "\n",
    "        # On vérifie que les actions utilisées sont déclarées\n",
    "        for action in actions_utilisés:\n",
    "            if action not in self.actions:\n",
    "                print(f\"Warning : l'action {action} est utilisée mais n'est pas déclarée\")\n",
    "                Warning = True\n",
    "                if resolution:\n",
    "                    print(\"Rajout de l'action manquante\")\n",
    "                    # On rajoute l'action manquante\n",
    "                    if self.actions == [\"None\"]:\n",
    "                        self.actions = [action]\n",
    "                    else:\n",
    "                        self.actions.append(action)\n",
    "\n",
    "        if Warning :\n",
    "            print(\"\\n Écrire ok pour contiuer\")\n",
    "            print(\"-\"*25 + \"\\n\")\n",
    "            while True:\n",
    "                rep = input()\n",
    "                if rep == \"ok\":\n",
    "                    break\n",
    "            \n",
    "    def affichage_etat(self,etat_actif,choix = False, reward = False):\n",
    "        \"\"\"\n",
    "        Affiche proprement l'état actif de la chaine de markov et permet de choisir une action le cas échéant\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        etat_actif : état de la chaine de markov\n",
    "            Etat actif de la chaine de markov\n",
    "        choix : bool\n",
    "            Si on doit afficher des choix pour des actions\n",
    "        \"\"\"\n",
    "        nom = etat_actif.state\n",
    "        print(\"-\"*50)\n",
    "        chaine_nom = f\"| Etat actif : {nom}\"\n",
    "        l = len(chaine_nom)\n",
    "        print(chaine_nom + \" \"*(49-l) + \"|\")\n",
    "        if choix :\n",
    "            chaine_choix = f\"| choix possibles pour l'état {etat_actif.state}:\"\n",
    "            l = len(chaine_choix)\n",
    "            print(chaine_choix + \" \"*(49-l) + \"|\")\n",
    "\n",
    "            chaine_transi = f\"| {list(etat_actif.transitions_with_action.keys())}\"\n",
    "            l = len(chaine_transi)\n",
    "            print(chaine_transi + \" \"*(49-l) + \"|\")\n",
    "\n",
    "            chaine_action = f\"| choix de l'action  :\"\n",
    "            l = len(chaine_action)\n",
    "            print(chaine_action + \" \"*(49-l) + \"|\")\n",
    "            time.sleep(0.5)\n",
    "            choix = input()\n",
    "            while choix not in list(etat_actif.transitions_with_action.keys()): # on vérifie que l'action choisie est valide\n",
    "                choix = input()\n",
    "                if choix == \"stop\": # permet d'arrêter le programme à la main\n",
    "                    break\n",
    "            chaine_choix = f\"| Vous avez choisi l'action {choix}\"\n",
    "            l = len(chaine_choix)\n",
    "            print(chaine_choix + \" \"*(49-l) + \"|\")\n",
    "\n",
    "        elif reward:\n",
    "            chaine_choix = f\"| Récompense pour l'état {etat_actif.state}:\"\n",
    "            l = len(chaine_choix)\n",
    "            print(chaine_choix + \" \"*(49-l) + \"|\")\n",
    "            time.sleep(0.5)\n",
    "            choix = input()\n",
    "            chaine_rec = f\"| Vous avez choisi : {choix}\"\n",
    "            l = len(chaine_rec)\n",
    "            print(chaine_rec + \" \"*(49-l) + \"|\")\n",
    "\n",
    "            \n",
    "        print(\"-\"*50)\n",
    "        print(\"\\n\")   \n",
    "        return choix    \n",
    "\n",
    "    def parcours(self, file_name=\"visu_parcours\"): # on parcours la chaine (en faisant N étapes)\n",
    "        \"\"\" \n",
    "        On parcours la chaine en faisant N étapes\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        file_name : str\n",
    "            Nom du fichier de sauvegarde de la vidéo\n",
    "        \"\"\"\n",
    "        positionnel = False\n",
    "        n_pos = False\n",
    "        without_action = False\n",
    "        \n",
    "        print(\"Quel mode de parcours voulez-vous ?\")\n",
    "        print(\"1 : sans actions\")\n",
    "        print(\"2 : avec adversaire positionnel\")\n",
    "        print(\"3 : avec adversaire non positionnel\")\n",
    "        print(\"4 : avec adversaire choisi de manière aléatoire\")\n",
    "        time.sleep(0.5)\n",
    "        choix_parcours = input()\n",
    "        while choix_parcours not in [\"1\",\"2\",\"3\",\"4\"]:\n",
    "            choix_parcours = input()\n",
    "        if choix_parcours == \"1\":\n",
    "            without_action = True\n",
    "        elif choix_parcours == \"2\":\n",
    "            positionnel = True\n",
    "        elif choix_parcours == \"3\":\n",
    "            n_pos = True\n",
    "        elif choix_parcours == \"4\":\n",
    "            random_adv = True\n",
    "        \n",
    "        print(\"Combien d'étapes voulez-vous faire ?\")\n",
    "        time.sleep(0.5)\n",
    "        N = int(input())\n",
    "\n",
    "        etat_initial = self.liste_states[0] #état initial : premier élément\n",
    "        etat_actif = self.states[etat_initial] # on prend l'objet correspondant à l'état initial\n",
    "        reward_total = 0\n",
    "        \n",
    "        self.afficher(etat = etat_initial, file_name='image0')\n",
    "        images = []\n",
    "        images.append(imageio.imread('image0'+'.png'))\n",
    "\n",
    "        detection_erreur = False\n",
    "        if without_action:\n",
    "        # On vérifie qu'on ne choisit pas le mode \"sans actions\" alors qu'il y en a\n",
    "            for state in self.states:\n",
    "                if len(self.states[state].transitions_with_action) != 0:\n",
    "                    print(f\"Error : il y a des transitions avec actions dans l'état {state}, il faut choisir le mode avec actions\")\n",
    "                    without_action = False\n",
    "                    detection_erreur = True\n",
    "                    break\n",
    "            if detection_erreur:\n",
    "                print(\"Voulez-vous changer de mode ? (y/n, si non, le programme s'arrête)\")\n",
    "                time.sleep(0.5)\n",
    "                choix = input()\n",
    "                while choix not in [\"y\",\"n\"]:\n",
    "                    choix = input()\n",
    "                if choix == \"y\":\n",
    "                    print(\"Quel mode de parcours voulez-vous ?\")\n",
    "                    print(\"1 : avec adversaire positionnel\")\n",
    "                    print(\"2 : avec adversaire non positionnel\")\n",
    "                    print(\"3 : avec adversaire choisi de manière aléatoire\")\n",
    "                    time.sleep(0.5)\n",
    "                    choix_parcours = input()\n",
    "                    while choix_parcours not in [\"1\",\"2\",\"3\"]:\n",
    "                        choix_parcours = input()\n",
    "                    if choix_parcours == \"1\":\n",
    "                        positionnel = True\n",
    "                    elif choix_parcours == \"2\":\n",
    "                        n_pos = True\n",
    "                    elif choix_parcours == \"3\":\n",
    "                        random_adv = True\n",
    "                else :\n",
    "                    return\n",
    "        \n",
    "        if without_action:\n",
    "            for i in range(N):\n",
    "                markov.affichage_etat(self, etat_actif)\n",
    "                # print(etat_actif.state) # on affiche l'état en cours\n",
    "                poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "                poids_total = np.sum(poids)\n",
    "                poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                for j in range(len(poids)): # on recherche l'état cible\n",
    "                    if choix <= poids[j]:\n",
    "                        reward_total += etat_actif.reward\n",
    "                        etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                        break\n",
    "                self.afficher(etat = etat_actif.state, file_name='image'+str(i+1))\n",
    "                images.append(imageio.imread('image'+str(i+1)+'.png'))\n",
    "            # create gif\n",
    "            imageio.mimsave(file_name+'.gif', images, fps=3)\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i)+'.png')\n",
    "\n",
    "        elif positionnel == True or n_pos == True: # adversaire positionnel\n",
    "            if positionnel :\n",
    "                print(\"Choix d'un adversaire positionnel\")\n",
    "                adv_pos = {} # contient pour chaque état, le choix de l'adversaire\n",
    "                for state in self.states :\n",
    "                    etat = self.states[state]\n",
    "                    if etat.transitions_with_action != {} : # si l'état possèdes des transitions avec actions\n",
    "                        action = markov.affichage_etat(self, etat, choix = True)\n",
    "                        adv_pos[state] = action\n",
    "\n",
    "            for i in range(N):\n",
    "                if etat_actif.transitions_with_action == {} : # si l'état n'a pas de transitions avec actions :\n",
    "                    markov.affichage_etat(self, etat_actif)\n",
    "                    poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "\n",
    "                    poids_total = np.sum(poids)\n",
    "                    poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                    poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                    choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                    for j in range(len(poids)): # on recherche l'état cible\n",
    "                        if choix <= poids[j]:\n",
    "                            reward_total += etat_actif.reward\n",
    "                            etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                            break\n",
    "                else :\n",
    "                    if n_pos == True :\n",
    "                        action_choisie = markov.affichage_etat(self, etat_actif, choix =True)\n",
    "                    else :\n",
    "                        action_choisie = adv_pos[etat_actif.state]\n",
    "                        print(f\"Action choisie : {action_choisie}\")\n",
    "                        markov.affichage_etat(self, etat_actif)\n",
    "                    poids = etat_actif.transitions_with_action[action_choisie][\"weights\"] # on ne prend que les poids de l'action choisie par l'adversaire\n",
    "                    poids_total = np.sum(poids)\n",
    "                    poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                    poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                    choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                    for j in range(len(poids)): # on recherche l'état cible\n",
    "                        if choix <= poids[j]:\n",
    "                            reward_total += etat_actif.reward\n",
    "                            etat_actif = self.states[etat_actif.transitions_with_action[action_choisie][\"targets\"][j]]\n",
    "                            break\n",
    "                        \n",
    "                self.afficher(etat = etat_actif.state, file_name='image'+str(i+1))\n",
    "                images.append(imageio.imread('image'+str(i+1)+'.png'))\n",
    "            # create gif\n",
    "            imageio.mimsave(file_name+'.gif', images, fps=3)\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i)+'.png')\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i))\n",
    "                \n",
    "        elif random_adv == True :\n",
    "            for i in range(N):\n",
    "                    actions_possibles = []\n",
    "                    for action in etat_actif.transitions_with_action :  # on récupère les actions possibles\n",
    "                        actions_possibles.append(action)\n",
    "                    if len(actions_possibles) == 0 : # si l'état n'a pas de transitions avec actions :\n",
    "\n",
    "                        poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "\n",
    "                        poids_total = np.sum(poids)\n",
    "                        poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                        poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                        choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                        for j in range(len(poids)): # on recherche l'état cible\n",
    "                            if choix <= poids[j]:\n",
    "                                reward_total += etat_actif.reward\n",
    "                                etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                                break\n",
    "                    else :\n",
    "                        action_choisie = random.choice(actions_possibles) # on choisit une action aléatoire\n",
    "                        poids = etat_actif.transitions_with_action[action_choisie][\"weights\"] # on ne prend que les poids de l'action choisie par l'adversaire\n",
    "                        poids_total = np.sum(poids)\n",
    "                        poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "                        poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "                        choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "                        for j in range(len(poids)): # on recherche l'état cible\n",
    "                            if choix <= poids[j]:\n",
    "                                reward_total += etat_actif.reward\n",
    "                                etat_actif = self.states[etat_actif.transitions_with_action[action_choisie][\"targets\"][j]]\n",
    "                                break\n",
    "                                           \n",
    "                    self.afficher(etat = etat_actif.state, file_name='image'+str(i+1))\n",
    "                    images.append(imageio.imread('image'+str(i+1)+'.png'))\n",
    "            imageio.mimsave(file_name+'.gif', images,fps=3)\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i)+'.png')\n",
    "            for i in range(N+1):\n",
    "                os.remove('image'+str(i))\n",
    "\n",
    "        print(f\"Reward total : {reward_total}\")\n",
    "        print(\"Le graphique est visible dans le fichier visu_parcours.gif\")\n",
    "    \n",
    "\n",
    "    def parcours_SMC(self, length, etat): # on parcours la chaine (en faisant N étapes)\n",
    "        \"\"\" \n",
    "        On parcours la chaine sans les actions, sans afficher le graphe. \n",
    "        \n",
    "        Args:\n",
    "            length (int): nombre max d'itération \n",
    "            etat (str): état d'arrivée\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        etat_initial = self.liste_states[0] #état initial : premier élément\n",
    "        etat_actif = self.states[etat_initial] # on prend l'objet correspondant à l'état initial\n",
    "        reward_total = 0\n",
    "        if etat_actif.state == etat:\n",
    "            return etat\n",
    "                        \n",
    "        for i in range(length):\n",
    "            # print(etat_actif.state) # on affiche l'état en cours\n",
    "            poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "            poids_total = np.sum(poids)\n",
    "            poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "            poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "            choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "            for j in range(len(poids)): # on recherche l'état cible\n",
    "                if choix <= poids[j]:\n",
    "                    reward_total += etat_actif.reward\n",
    "                    etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                    break\n",
    "            if etat_actif.state == etat:\n",
    "                break\n",
    "            \n",
    "        return etat_actif.state\n",
    "\n",
    "\n",
    "    \n",
    "    def afficher(self, etat = \"default\", file_name = \"graph\"):\n",
    "        \n",
    "        \"\"\"Fonction qui permet de représenter le graphe de la chaine de Markov avec ou sans action. Dans le mode \"default\", le graphe de base est affiché.\n",
    "        Dans le mode \"etat\", on peut choisir un état et il sera mis en évidence en étant de couleur bleue.\n",
    "         \"\"\"\n",
    "    \n",
    "        # On récupère les données\n",
    "        States = self.states\n",
    "        Actions = self.actions\n",
    "        Transitions_with_action = self.transitions_with_action\n",
    "        Transitions_without_action = self.transitions_without_action\n",
    "\n",
    "        # On crée le graphique\n",
    "        G = gv.Digraph(format='png')\n",
    "\n",
    "        if etat == \"default\":\n",
    "            for state in States:\n",
    "                G.node(state)\n",
    "        else:\n",
    "            for state in States:\n",
    "                if state == etat:\n",
    "                    G.node(state, color = 'blue', style = 'filled')\n",
    "                else:\n",
    "                    G.node(state)\n",
    "        \n",
    "        #On ajoute les actions\n",
    "        for actions in Actions:\n",
    "            G.node(actions, shape = 'point')\n",
    "            \n",
    "        for transition in Transitions_with_action:\n",
    "            G.edge(Transitions_with_action[transition]['from'], Transitions_with_action[transition]['action'], label=str(Transitions_with_action[transition]['action']), color = 'red')\n",
    "        \n",
    "\n",
    "        # On ajoute les transitions sans action\n",
    "        for transition in Transitions_without_action:\n",
    "            for i in range(len(Transitions_without_action[transition]['targets'])):\n",
    "                G.edge(Transitions_without_action[transition]['from'],Transitions_without_action[transition]['targets'][i], label = str(Transitions_without_action[transition]['weights'][i])) \n",
    "\n",
    "        # On ajoute les transitions avec action\n",
    "        for transition in Transitions_with_action:\n",
    "            for i in range(len(Transitions_with_action[transition]['targets'])):\n",
    "                G.edge(Transitions_with_action[transition]['action'],Transitions_with_action[transition]['targets'][i], label = str(Transitions_with_action[transition]['weights'][i]))\n",
    "        \n",
    "        G.render(file_name, view=False)\n",
    "\n",
    "    def eventually_check_DTMC(self, S1):\n",
    "        \"\"\" Model Checking du eventually pour une chaine de Markov sans action.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        S1 : list\n",
    "            Liste des états qu'on cherche à atteindre\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        S_int = [x for x in self.liste_states if x not in S1] # on crée la liste des étates S_? (\"int\" pour interrogation)\n",
    "        S0 = [] # on crée la liste des états S_0 qu'on remplira par récurrence\n",
    "        S_int_2 = S_int.copy()\n",
    "\n",
    "        for state in S_int : # Algorithme de recherche en profondeur qu'on applique à chaque état de S_int\n",
    "            pile = []\n",
    "            pile.append(state)\n",
    "            etats_marques = [state]\n",
    "            while pile != []:\n",
    "                etat = pile[-1]\n",
    "                etat_actif = self.states[etat]\n",
    "                if etat in S1 :\n",
    "                    break\n",
    "                etats_faisables = [etat for etat in etat_actif.transitions_without_action[\"targets\"] if etat not in etats_marques]\n",
    "                if etats_faisables != [] :\n",
    "                    pile.append(etats_faisables[0])\n",
    "                    etats_marques.append(etats_faisables[0])\n",
    "                else:\n",
    "                    pile.pop()\n",
    "            if pile == []:\n",
    "                S_int_2.remove(state)\n",
    "                S0.append(state) # on ajoute l'état à S0 si on ne peut pas atteindre un état de S1\n",
    "\n",
    "        print(f\"Les états S_0 sont : {S0}\")\n",
    "        print(f\"Les états S_? sont : {S_int_2}\")\n",
    "        print(f\"Les états S_1 sont : {S1}\")\n",
    "\n",
    "        A = np.zeros((len(S_int_2),len(S_int_2))) # Création de la matrice A\n",
    "        for i in range(len(S_int_2)):\n",
    "            etat_actif = self.states[S_int_2[i]]\n",
    "            for j in range(len(S_int_2)):\n",
    "                if S_int_2[j] in etat_actif.transitions_without_action[\"targets\"]:\n",
    "                    index = etat_actif.transitions_without_action[\"targets\"].index(S_int_2[j])\n",
    "                    A[i,j] = etat_actif.transitions_without_action[\"weights\"][index]/(np.sum(etat_actif.transitions_without_action[\"weights\"])) # on remplit la matrice A avec les bons poids\n",
    "\n",
    "        b = np.zeros((len(S_int_2),1)) # Création du vecteur b\n",
    "        for i in range(len(S_int_2)):\n",
    "            etat_actif = self.states[S_int_2[i]]\n",
    "            for transitions in etat_actif.transitions_without_action[\"targets\"]:\n",
    "                if transitions in S1:\n",
    "                    index = etat_actif.transitions_without_action[\"targets\"].index(transitions)\n",
    "                    b[i,0] += etat_actif.transitions_without_action[\"weights\"][index]/(np.sum(etat_actif.transitions_without_action[\"weights\"])) # on remplit le vecteur b avec les bons poids\n",
    "\n",
    "        # On résout (Id-A)^-1 * b\n",
    "        Id = np.identity(len(S_int_2))\n",
    "        y = np.dot(np.linalg.inv(Id - A),b)\n",
    "\n",
    "        print(f\"La probabilité de respecter la propriété à partir de l'état initial {self.liste_states[0]} est : {y[0,0]}\")\n",
    "        print(\" vecteur y total :\")\n",
    "        print(y)\n",
    "\n",
    "    def smc_quantitatif(self):\n",
    "        print(\"-\"*50)\n",
    "        print(\"Quel sont les états cibles ? Veuillez les rentrer un par un, et taper 'fin' quand vous avez fini\")\n",
    "        time.sleep(0.5)\n",
    "        etat = input()\n",
    "        etats_cibles = []\n",
    "        while etat != \"fin\":\n",
    "            if etat not in self.liste_states:\n",
    "                print(\"L'état\", etat, \"n'existe pas\")\n",
    "                etat = input()\n",
    "            else :\n",
    "                etats_cibles.append(etat)\n",
    "                etat = input()\n",
    "        print(\"Etats cibles : \", etats_cibles)\n",
    "        print(\"Précision souhaitée :\")\n",
    "        time.sleep(0.5)\n",
    "        precision = float(input())\n",
    "        print(\"Précision choisie : \", precision)\n",
    "        print(\"Erreur souhaitée :\")\n",
    "        time.sleep(0.5)\n",
    "        erreur = float(input())\n",
    "        print(\"Erreur choisie : \", erreur)\n",
    "        print(\"Longueur des chaines voulues :\")\n",
    "        time.sleep(0.5)\n",
    "        longueur = int(input())\n",
    "        print(\"Longueur des chaines : \", longueur)\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        # Calcul de N (nombre d'itérations)\n",
    "        N =int((np.log(2)-np.log(erreur))/(2*precision)**2)+1\n",
    "\n",
    "        print(\"Calcul en cours.. \\n\")\n",
    "        # On lance un certain nombre d'itération pour chaque état cible\n",
    "        res_prob = [] # vecteur des probabilités d'atteindre les étates\n",
    "        for i in range(len(etats_cibles)): \n",
    "            lancers = [markov.parcours_SMC(self, longueur, etats_cibles[i]) for _ in range(N)]\n",
    "            res_prob.append(round(sum([1 for x in lancers if x==etats_cibles[i]])/N,5))\n",
    "        print(f\"Les probabilité d'atteindre chacun des états cibles {etats_cibles} sont : {res_prob}\")\n",
    "\n",
    "        return\n",
    "\n",
    "    def smc_qualitatif(self):\n",
    "        print(\"-\"*50)\n",
    "        print(\"Quel est l'état cible ?\")\n",
    "        time.sleep(0.5)\n",
    "        etat = input()\n",
    "        while etat not in self.liste_states:\n",
    "            print(\"L'état\", etat, \"n'existe pas\")\n",
    "            etat = input()\n",
    "        print(\"Etat cible : \", etat)\n",
    "        print(\"Valeur de alpha :\")\n",
    "        time.sleep(0.5)\n",
    "        alpha = float(input())\n",
    "        print(\"alpha = \", alpha)\n",
    "        print(\"Valeur de beta :\")\n",
    "        time.sleep(0.5)\n",
    "        beta = float(input())\n",
    "        print(\"beta = \", beta)\n",
    "        print(\"Valeur de théta :\")\n",
    "        time.sleep(0.5)\n",
    "        theta = float(input())\n",
    "        print(\"théta = \", theta)\n",
    "        print(\"Valeur de epsilon (zone d'indifférence) :\")\n",
    "        time.sleep(0.5)\n",
    "        epsilon = float(input())\n",
    "        print(\"epsilon = \", epsilon)\n",
    "        print(\"Longueur des chaines voulues :\")\n",
    "        time.sleep(0.5)\n",
    "        longueur = int(input())\n",
    "        print(\"Longueur des chaines = \", longueur)\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        print(\"Calcul en cours.. \\n\")\n",
    "\n",
    "        gamma0 = theta + epsilon\n",
    "        gamma1 = theta - epsilon\n",
    "        A = (1-beta)/alpha\n",
    "        B = beta/(1-alpha)\n",
    "        Vadd = np.log(gamma1/gamma0)\n",
    "        Vrem = np.log((1-gamma1)/(1-gamma0))\n",
    "        FA = np.log(A)\n",
    "        FB = np.log(B)\n",
    "        Fm = 0\n",
    "        m=0 # nombtre de lancers\n",
    "        dm=0 # nombre de lancers acceptés\n",
    "        while True:\n",
    "            lancer = markov.parcours_SMC(self, longueur, etat)\n",
    "            m+=1\n",
    "            if  lancer == etat:\n",
    "                Fm = Fm + Vadd\n",
    "                dm+=1\n",
    "            else :\n",
    "                Fm = Fm + Vrem\n",
    "            if Fm > FA :\n",
    "                print(f\"La probabilité d'atteindre l'état {etat} est inférieure à {theta}\")\n",
    "                print(\"Le nombre de simulations nécessaires pour démontrer ce résultat est de\", m)\n",
    "                break\n",
    "            elif Fm < FB :\n",
    "                print(f\"La probabilité d'atteindre l'état {etat} est bien supérieure ou égal à {theta}\")\n",
    "                print(\"Le nombre de simulations nécessaires pour démontrer ce résultat est de\", m)\n",
    "                break\n",
    "\n",
    "    def next_check(self):\n",
    "        print(\"-\"*50)\n",
    "        print(\"Quel sont les états cibles ? Veuillez les rentrer un par un, et taper 'fin' quand vous avez fini\")\n",
    "        time.sleep(0.5)\n",
    "        etat = input()\n",
    "        etats_cibles = []\n",
    "        while etat != \"fin\":\n",
    "            if etat not in self.liste_states:\n",
    "                print(\"L'état\", etat, \"n'existe pas\")\n",
    "                etat = input()\n",
    "            else :\n",
    "                etats_cibles.append(etat)\n",
    "                etat = input()\n",
    "        print(\"Les états cibles sont :\", etats_cibles)\n",
    "\n",
    "        print(\"Quel est l'état initial du calcul du next ?\")\n",
    "        time.sleep(0.5)\n",
    "        etat_initial = input()\n",
    "        while etat_initial not in self.liste_states:\n",
    "            print(\"L'état\", etat, \"n'existe pas\")\n",
    "            etat_initial = input()\n",
    "        print(\"L'état initial est :\", etat_initial)\n",
    "        print(\"-\"*50)\n",
    "        print(\"Calcul en cours.. \\n\")\n",
    "\n",
    "        probas = 0\n",
    "        etat_actif = self.states[etat_initial]\n",
    "        if etat_actif.transitions_without_action != {}:\n",
    "            for i,etat in enumerate(etat_actif.transitions_without_action[\"targets\"]):\n",
    "                if etat in etats_cibles:\n",
    "                    probas += etat_actif.transitions_without_action[\"weights\"][i]\n",
    "            print(\"Probabilité du next à partir de l'état initial :\", probas/(np.sum(etat_actif.transitions_without_action[\"weights\"])))\n",
    "        else :\n",
    "            actions_possibles = []\n",
    "            for action in etat_actif.transitions_with_action :  # on récupère les actions possibles pour l'état actif\n",
    "                actions_possibles.append(action)\n",
    "            res_matrice = np.empty((len(actions_possibles),2), dtype = object)\n",
    "            res_matrice[:,0] = actions_possibles\n",
    "            for action in actions_possibles:\n",
    "                probas = 0\n",
    "                for i,etat in enumerate(etat_actif.transitions_with_action[action][\"targets\"]):\n",
    "                    if etat in etats_cibles:\n",
    "                        probas += etat_actif.transitions_with_action[action][\"weights\"][i]\n",
    "                res_matrice[i,1] = probas/probas/(np.sum(etat_actif.transitions_with_action[action][\"weights\"]))\n",
    "            print(f\"Probabilité du next à partir de l'état {etat_initial} pour chaque action possible:\")\n",
    "            print(res_matrice)\n",
    "\n",
    "        \n",
    "    def calcul_max(self,etat_actif,V0,gamma):\n",
    "        max = etat_actif.reward\n",
    "        # print(f\"Etat actif {etat_actif.state}reward {etat_actif.reward}\")\n",
    "        max_action = None\n",
    "        somme = 0\n",
    "        \n",
    "        actions_possibles = []\n",
    "        for action in etat_actif.transitions_with_action :  # on récupère les actions possibles\n",
    "            actions_possibles.append(action)\n",
    "\n",
    "        if actions_possibles != []: #Si l'état possèdes des transitions avec actions\n",
    "            for action in actions_possibles:\n",
    "                for i,etat in enumerate(etat_actif.transitions_with_action[action][\"targets\"]): # Si l'état possède des transitions avec une action\n",
    "                    # On calcule la probabilité de transition de l'état actif vers l'état i en passant par l'action action\n",
    "                    poids = etat_actif.transitions_with_action[action][\"weights\"][i]/np.sum(etat_actif.transitions_with_action[action][\"weights\"])\n",
    "                    somme += poids*V0[self.liste_states.index(etat)]\n",
    "                if max < etat_actif.reward + gamma*somme:\n",
    "                    max = etat_actif.reward + gamma*somme\n",
    "                    max_action = action\n",
    "                somme = 0\n",
    "        else :\n",
    "            for i,etat in enumerate(etat_actif.transitions_without_action[\"targets\"]): # Si l'état possède des transitions avec une action\n",
    "                # On calcule la probabilité de transition de l'état actif vers l'état i en passant par l'action action\n",
    "                poids = etat_actif.transitions_without_action[\"weights\"][i]/np.sum(etat_actif.transitions_without_action[\"weights\"])\n",
    "                somme += poids*V0[self.liste_states.index(etat)]\n",
    "\n",
    "            if max < etat_actif.reward + gamma*somme:\n",
    "                max = etat_actif.reward + gamma*somme\n",
    "                max_action = None\n",
    "        return(max, max_action)\n",
    "\n",
    "    \n",
    "    def algos_iterations(self):\n",
    "            \"\"\"\n",
    "            Calcul de meilleur adversaire à partir d'un algorithme par itérations\n",
    "            \"\"\"\n",
    "            print(\"-\"*50)\n",
    "            print(\"Choix de gamma :\")\n",
    "            time.sleep(0.5)\n",
    "            gamma = float(input())\n",
    "            print(\"Gamma choisi :\", gamma)\n",
    "            print(\"Choix de epsilon :\")\n",
    "            time.sleep(0.5)\n",
    "            epsilon = float(input())\n",
    "            print(\"Epsilon choisi :\", epsilon)\n",
    "            print(\"Nombre d'itérations avant arrêt forcé (au cas où il n'y pas de convergence) :\")\n",
    "            time.sleep(0.5)\n",
    "            nb_iter_max = int(input())\n",
    "            print(\"Nombre d'itérations max :\", nb_iter_max)\n",
    "            print(\"-\"*50)\n",
    "            print(\"Calcul de Vn par itérations...\\n\")\n",
    "            # Initialisation du vecteur\n",
    "            V0 = np.zeros((len(self.liste_states),1))\n",
    "            V1 = V0 + 2*epsilon # Pour être sur de ne pas sortir de la boucle à la première itération \n",
    "            nb_iter=0\n",
    "            while True and nb_iter < nb_iter_max:\n",
    "                for i,etat in enumerate(self.liste_states):\n",
    "                    etat_actif = self.states[etat]\n",
    "                    V1[i], _ = markov.calcul_max(self,etat_actif, V0, gamma)\n",
    "                nb_iter+=1\n",
    "                if np.linalg.norm(V1-V0) < epsilon:\n",
    "                    break\n",
    "                else :\n",
    "                    V0 = np.copy(V1)\n",
    "            print(\"-\"*50)\n",
    "            if nb_iter == nb_iter_max:\n",
    "                print(\"Le nombre d'itérations maximum a été atteint\")\n",
    "            print(\"Valeur de Vn :\")\n",
    "            print(V1)\n",
    "            print(\"Nombre d'itérations nécessaires pour la couvergence (s'il y a convergence): \", i)\n",
    "            print(\"-\"*50 + \"\\n\")\n",
    "            adv_choisi = np.empty((len(self.liste_states),2), dtype = object)\n",
    "            adv_choisi[:,0] = self.liste_states\n",
    "            for i,etat in enumerate(self.liste_states):\n",
    "                etat_actif = self.states[etat]\n",
    "                _, max_action = markov.calcul_max(self,etat_actif, V0, gamma)\n",
    "                adv_choisi[i,1] = max_action\n",
    "            \n",
    "            print(\"-\"*50)\n",
    "            print(\"Adversaire optimal choisi (si None, c'est qu'il n'y a pas d'actions de disponibles) : \")\n",
    "            print(adv_choisi)\n",
    "            print(\"-\"*50 + \"\\n\")\n",
    "            return\n",
    "\n",
    "    def parcours_q_learning(self, etat, action_param):\n",
    "        \"\"\"\n",
    "        Fonction de parcours particulière pour le q_learning\n",
    "        On ne fait qu'une étape à chaque fois, en partant d'un état et d'une action en particulier\n",
    "\n",
    "        Args\n",
    "        ----\n",
    "        etat : str\n",
    "            état de départ (nom de l'état)\n",
    "        action : str\n",
    "            action à effectuer (nom de l'action)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        etat_suivant : str\n",
    "            état d'arrivée\n",
    "        reward : float\n",
    "            reward associé à l'état dont on sort\n",
    "        \"\"\"\n",
    "\n",
    "        etat_initial = self.liste_states[self.liste_states.index(etat)] #état initial : état donné en paramètre\n",
    "        etat_actif = self.states[etat_initial] # on prend l'objet correspondant à l'état initial     \n",
    "\n",
    "        actions_possibles = []\n",
    "        for action in etat_actif.transitions_with_action :  # on récupère les actions possibles\n",
    "            actions_possibles.append(action)\n",
    "        if len(actions_possibles) == 0 : # si l'état n'a pas de transitions avec actions :\n",
    "            poids = etat_actif.transitions_without_action[\"weights\"]\n",
    "\n",
    "            poids_total = np.sum(poids)\n",
    "            poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "            poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "            choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "            for j in range(len(poids)): # on recherche l'état cible\n",
    "                if choix <= poids[j]:\n",
    "                    reward_total = etat_actif.reward\n",
    "                    etat_actif = self.states[etat_actif.transitions_without_action[\"targets\"][j]]\n",
    "                    break\n",
    "        else :\n",
    "            if action_param not in actions_possibles : # si l'action n'est pas possible, on choisit une action aléatoire\n",
    "                action_choisie = random.choice(actions_possibles)\n",
    "            else :\n",
    "                action_choisie = action_param # on choisit l'action passée en argument\n",
    "            \n",
    "            poids = etat_actif.transitions_with_action[action_choisie][\"weights\"] # on ne prend que les poids de l'action choisie par l'adversaire\n",
    "            poids_total = np.sum(poids)\n",
    "            poids = poids/poids_total # on normalise les poids pour qu'ils soient entre 0 et 1\n",
    "            poids = np.cumsum(poids) # on fait la somme cumulée des poids, afin de pouvoir faire un tirage aléatoire (il ne faut pas que par exemple les deux probas soient de 0.5, il en faut une de 0.5 et l'autre de 1)\n",
    "\n",
    "            choix = random.random() # tirage aléatoire entre 0 et 1\n",
    "\n",
    "            for j in range(len(poids)): # on recherche l'état cible\n",
    "                if choix <= poids[j]:\n",
    "                    reward_total = etat_actif.reward\n",
    "                    etat_actif = self.states[etat_actif.transitions_with_action[action_choisie][\"targets\"][j]]\n",
    "                    break\n",
    "        return etat_actif.state, reward_total\n",
    "\n",
    "    def q_learning(self):\n",
    "        \"\"\"\n",
    "        Implémentation du Q learning\n",
    "        \"\"\"\n",
    "        print(\"-\"*50)\n",
    "        print(\"Choix de gamma :\")\n",
    "        time.sleep(0.5)\n",
    "        gamma = float(input())\n",
    "        print(\"Gamma choisi :\", gamma)\n",
    "        print(\"Choix du nombre d'itérations total (Ttot) :\")\n",
    "        time.sleep(0.5)\n",
    "        Ttot = int(input())\n",
    "        print(\"Ttot choisi :\", Ttot)\n",
    "        print(\"Souhaitez-vous l'affichage de la matrice Q ? (y/n)\")\n",
    "        time.sleep(0.5)\n",
    "        affichage = input()\n",
    "        while affichage != \"y\" and affichage != \"n\":\n",
    "            affichage = input()\n",
    "        print(\"-\"*50 + \"\\n\")\n",
    "\n",
    "        print(\"Calcul en cours...\\n\")\n",
    "        print(\"-\"*50)\n",
    "\n",
    "        etats = self.liste_states\n",
    "        actions = self.actions\n",
    "        alpha_tab = np.zeros((len(etats),len(actions))) # alpha est une fonction décroissante (plus on a rencontré un état, plus changements effectués seront faibles)\n",
    "        Q = np.zeros((len(etats),len(actions))) # Création de la matrice Q\n",
    "        st = self.liste_states[0]\n",
    "        st_next = st\n",
    "        DejaVu = False\n",
    "        for t in range(Ttot):\n",
    "            if st_next == st and DejaVu == True: # on a déjà vu cet état deux fois d'affilées, on change d'état\n",
    "                st = \"S0\"\n",
    "                DejaVu = False\n",
    "            elif st_next == st and DejaVu == False:\n",
    "                DejaVu = True\n",
    "            else :\n",
    "                DejaVu = False\n",
    "                st = st_next\n",
    "            if random.random() <0.9: # dilemme exploitation/exploration\n",
    "                i = np.argmax(Q[etats.index(st),:]) # Dans la plupart des cas, ou prend la \"meilleure\" action\n",
    "                at = actions[i]\n",
    "            else :\n",
    "                at = random.choice(actions) # on choisit une action au hasard dans qq cas\n",
    "            (st_next,rt) = markov.parcours_q_learning(self,st,at) # On effectue le mouvement\n",
    "            etat_index = etats.index(st)\n",
    "            action_index = actions.index(at)\n",
    "\n",
    "            delta = rt + gamma*max(Q[etats.index(st_next),:]) - Q[etat_index,action_index]\n",
    "            alpha_tab[etat_index,action_index]+=1\n",
    "            Q[etat_index,action_index] += (1/alpha_tab[etat_index,action_index])*delta\n",
    "        \n",
    "        if affichage == \"y\":\n",
    "            print(\"Matrice Q :\")\n",
    "            print(Q)\n",
    "\n",
    "        # On fait l'argmax sur chaque ligne de la matrice Q pour avoir le choix optimal pour chaque état\n",
    "        mat_choix = np.argmax(Q, axis=1)\n",
    "        res_choix = np.empty(dtype=object, shape=(len(etats),2))\n",
    "        for i in range(len(etats)):\n",
    "            # if etats[i] has no transitions with actions :\n",
    "            if len(self.states[etats[i]].transitions_with_action) == 0:\n",
    "                res_choix[i,1] = \"None\"\n",
    "            else :\n",
    "                res_choix[i,1] = actions[mat_choix[i]]\n",
    "        res_choix[:,0] = etats\n",
    "        print(\"Choix optimal pour chaque état (Si None c'est qu'il n'y pas de transitions avec actions pour l'état) :\")\n",
    "        print(res_choix)\n",
    "        print(\"-\"*50)\n",
    "    \n",
    "        return Q\n",
    "    \n",
    "        \n",
    "    def eventually_check_MDP(self,S1) : \n",
    "        \"\"\" Model Checking du eventually pour une chaine de Markov avec action.\n",
    "        \n",
    "        Args\n",
    "        ----\n",
    "        S1 : list\n",
    "            Liste des états qu'on cherche à atteindre\n",
    "        \"\"\"\n",
    "        n_etat = len(self.liste_states) - len(S1) # On ne compte pas les états qu'on veut atteindre \n",
    "        n_action = [] # Nombre d'actions possibles pour chaque état\n",
    "        \n",
    "        for i in range(n_etat):\n",
    "            n_action.append(len(self.states[self.liste_states[i]].transitions_with_action))\n",
    "        \n",
    "        n_tot_ligne = 0 \n",
    "        for i in range(n_etat):\n",
    "            if n_action[i] != 0:\n",
    "                n_tot_ligne += n_action[i] + 2\n",
    "            else:\n",
    "                n_tot_ligne += 3\n",
    "            \n",
    "        # On crée la matrice A (avec n_etat colonnes et n_etat*(n_action+2) lignes)\n",
    "        A = np.zeros((n_tot_ligne,n_etat))\n",
    "        \n",
    "        # On crée le vecteur b \n",
    "        b = np.zeros((n_tot_ligne,1))\n",
    "        \n",
    "        # On remplit la matrice A et le vecteur b\n",
    "\n",
    "        # Pour chaque bloc de taille (n_action[i]+2) lignes et n_etat colonnes on remplit la matrice A \n",
    "        indice = 0\n",
    "        for i in range(n_etat):\n",
    "            etat_actif = self.states[self.liste_states[i]]\n",
    "            # noms des actions possibles pour l'état i\n",
    "            if n_action[i] != 0:\n",
    "                for j in range(n_action[i]):\n",
    "                    nom_action = list(etat_actif.transitions_with_action.keys())[j]\n",
    "                    for k in range(n_etat):\n",
    "                        etat_cible = self.states[self.liste_states[k]]\n",
    "                        index_cible = -10\n",
    "                        if etat_cible.state in etat_actif.transitions_with_action[nom_action][\"targets\"]:\n",
    "                            index_cible = self.states[self.liste_states[i]].transitions_with_action[nom_action][\"targets\"].index(etat_cible.state)\n",
    "                        if k == i:\n",
    "                            if index_cible != -10:\n",
    "                                A[indice+j,k] = 1 - self.states[self.liste_states[i]].transitions_with_action[nom_action][\"weights\"][index_cible]  / np.sum(self.states[self.liste_states[i]].transitions_with_action[nom_action][\"weights\"])\n",
    "                            else:\n",
    "                                A[indice+j,k] = 1\n",
    "                        else:\n",
    "                            if index_cible != -10:    \n",
    "                                A[indice+j,k] = - self.states[self.liste_states[i]].transitions_with_action[nom_action][\"weights\"][index_cible]  / np.sum(self.states[self.liste_states[i]].transitions_with_action[nom_action][\"weights\"])\n",
    "                            else:\n",
    "                                A[indice+j,k] = 0\n",
    "                for transitions in etat_actif.transitions_with_action[nom_action][\"targets\"]:\n",
    "                    if transitions in S1:\n",
    "                        index_cible = etat_actif.transitions_with_action[nom_action][\"targets\"].index(transitions)\n",
    "                        b[indice+j] = self.states[self.liste_states[i]].transitions_with_action[nom_action][\"weights\"][index_cible]  / np.sum(self.states[self.liste_states[i]].transitions_with_action[nom_action][\"weights\"])\n",
    "                indice += n_action[i]\n",
    "                \n",
    "            else:\n",
    "                for j in range(1):\n",
    "                    if j == 0:\n",
    "                        for k in range(n_etat):\n",
    "                            etat_cible = self.states[self.liste_states[k]]\n",
    "                            index_cible = -10\n",
    "                            if etat_cible.state in etat_actif.transitions_without_action[\"targets\"]:\n",
    "                                index_cible = self.states[self.liste_states[i]].transitions_without_action[\"targets\"].index(etat_cible.state)\n",
    "                            if k == i:\n",
    "                                if index_cible != -10:\n",
    "                                    A[indice+j,k] = 1 - self.states[self.liste_states[i]].transitions_without_action[\"weights\"][index_cible]  / np.sum(self.states[self.liste_states[i]].transitions_without_action[\"weights\"])\n",
    "                                else:\n",
    "                                    A[indice+j,k] = 1\n",
    "                            else:\n",
    "                                if index_cible != -10:    \n",
    "                                    A[indice+j,k] = - self.states[self.liste_states[i]].transitions_without_action[\"weights\"][index_cible]  / np.sum(self.states[self.liste_states[i]].transitions_without_action[\"weights\"])\n",
    "                                else:\n",
    "                                    A[indice+j,k] = 0\n",
    "                    for transitions in etat_actif.transitions_without_action[\"targets\"]:\n",
    "                        if transitions in S1:\n",
    "                            index_cible = etat_actif.transitions_without_action[\"targets\"].index(transitions)\n",
    "                            b[indice+j] = self.states[self.liste_states[i]].transitions_without_action[\"weights\"][index_cible]  / np.sum(self.states[self.liste_states[i]].transitions_without_action[\"weights\"])\n",
    "                indice += 1\n",
    "\n",
    "            A[indice,i] = -1\n",
    "            A[indice+1,i] = 1\n",
    "            b[indice] = -1\n",
    "            indice += 2\n",
    "        \n",
    "        print(A)\n",
    "        print(b)\n",
    "        A = -1*A\n",
    "        b = -1*b\n",
    "                    \n",
    "        # On résout Ax>=b   avec comme fonction objectif (1,1,...,1)\n",
    "        c = np.ones((n_etat,1))\n",
    "        res = linprog(c, A_ub=A, b_ub=b, bounds=(None,None), method='simplex')\n",
    "        print(f\"Bravo ! L'algorithme converge ! La probabilité d'atteindre l'ensemble à partir des autre sommets est : \\n\")\n",
    "        liste_res = []\n",
    "        for etat in self.liste_states:\n",
    "            if etat not in S1:\n",
    "                liste_res.append(etat)\n",
    "        for etat in liste_res:\n",
    "            print(f\"Etat {etat} : {res.x[self.liste_states.index(etat)]}\")\n",
    "\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"States : {self.states} \\n Actions : {self.actions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.12.0\n",
      "ANTLR runtime and generated code versions disagree: 4.11.1!=4.12.0\n",
      "States: ['S0', 'S1', 'S2', 'S3', 'S4']\n",
      "Actions: ['a', 'b']\n",
      "Transition from S0 with action a and targets ['S1', 'S2'] with weights [5, 5]\n",
      "Transition from S0 with action b and targets ['S3', 'S4'] with weights [1, 9]\n",
      "Transition from S1 with no action and targets ['S1'] with weights [10]\n",
      "Transition from S3 with no action and targets ['S0'] with weights [10]\n",
      "Transition from S2 with no action and targets ['S0'] with weights [10]\n",
      "Transition from S4 with no action and targets ['S0'] with weights [10]\n",
      "{'States': ['S0', 'S1', 'S2', 'S3', 'S4'], 'Actions': ['a', 'b'], 'Transitions_with_action': {0: {'from': 'S0', 'action': 'a', 'targets': ['S1', 'S2'], 'weights': [5, 5]}, 1: {'from': 'S0', 'action': 'b', 'targets': ['S3', 'S4'], 'weights': [1, 9]}}, 'Transitions_without_action': {2: {'from': 'S1', 'targets': ['S1'], 'weights': [10]}, 3: {'from': 'S3', 'targets': ['S0'], 'weights': [10]}, 4: {'from': 'S2', 'targets': ['S0'], 'weights': [10]}, 5: {'from': 'S4', 'targets': ['S0'], 'weights': [10]}}, 'Rewards': {'S0': 0, 'S1': 5, 'S2': 100, 'S3': 500, 'S4': 3}}\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "Souhaitez-vous que les problèmes détéctés prochainement soient résolus automatiquement si cela est possible? (y/n)\n",
      "\n",
      "La chaine de markov a été initialisée, il est possible de la voir dans le fichier initial.png \n",
      "\n",
      "Que souhaitez-vous faire ?\n",
      "1. lancer un parcours\n",
      "2. model checking d'un eventually\n",
      "3. SMC quantitatif\n",
      "4. SMC qualitatif\n",
      "5. model checking d'un next\n",
      "6. Algorithme d'itération de valeurs\n",
      "7. Q learning\n",
      "\n",
      "Quel sont les états cibles ? Veuillez les rentrer un par un, et taper 'fin' quand vous avez fini\n",
      "[[ 1.  -0.5 -0.5  0. ]\n",
      " [ 1.   0.   0.  -0.1]\n",
      " [-1.   0.   0.   0. ]\n",
      " [ 1.   0.   0.   0. ]\n",
      " [ 0.   0.   0.   0. ]\n",
      " [ 0.  -1.   0.   0. ]\n",
      " [ 0.   1.   0.   0. ]\n",
      " [-1.   0.   1.   0. ]\n",
      " [ 0.   0.  -1.   0. ]\n",
      " [ 0.   0.   1.   0. ]\n",
      " [-1.   0.   0.   1. ]\n",
      " [ 0.   0.   0.  -1. ]\n",
      " [ 0.   0.   0.   1. ]]\n",
      "[[ 0. ]\n",
      " [ 0.9]\n",
      " [-1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [-1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [-1. ]\n",
      " [ 0. ]\n",
      " [ 0. ]\n",
      " [-1. ]\n",
      " [ 0. ]]\n",
      "Bravo ! L'algorithme converge ! La probabilité d'atteindre l'ensemble à partir des autre sommets est : \n",
      "\n",
      "Etat S0 : 1.0\n",
      "Etat S1 : 0.0\n",
      "Etat S2 : 1.0\n",
      "Etat S3 : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/04t34kkj27ldhv8wcvlpqz300000gn/T/ipykernel_22154/402046038.py:1147: DeprecationWarning: `method='simplex'` is deprecated and will be removed in SciPy 1.11.0. Please use one of the HiGHS solvers (e.g. `method='highs'`) in new code.\n",
      "  res = linprog(c, A_ub=A, b_ub=b, bounds=(None,None), method='simplex')\n"
     ]
    }
   ],
   "source": [
    "M = markov(fichier_mdp=\"chaines/casino.mdp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TP1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "40f7c2dd250ea64d1f119704b8cfa97c376b5923eda5d57d2493e68729089b32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
